{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ba6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "from tensorflow.keras.models import Sequential \n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "748da7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of concatenated images: (850, 400, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of folders\n",
    "folders = [\"C:/Users/rodea/OneDrive/Desktop/Dataset/J004\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J011\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J012\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J015\",\n",
    "           \"C:/Users/rodea/OneDrive/Desktop/Dataset/J016\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J021\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J023\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J024\",\n",
    "           \"C:/Users/rodea/OneDrive/Desktop/Dataset/J025\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J031\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J037\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J056\",\n",
    "           \"C:/Users/rodea/OneDrive/Desktop/Dataset/J058\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J065\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J066\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J069\",\n",
    "           \"C:/Users/rodea/OneDrive/Desktop/Dataset/J074\"]\n",
    "        \n",
    "          \n",
    "          \n",
    "\n",
    "# An empty list to store processed images\n",
    "train_images = []\n",
    "\n",
    "# Iterate through each folder\n",
    "for folder in folders:\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder)\n",
    "\n",
    "    # Iterate through each file\n",
    "    for file in files:\n",
    "        # Get the full file path\n",
    "        file_path = os.path.join(folder, file)\n",
    "\n",
    "        # Read the image\n",
    "        img = cv2.imread(file_path)\n",
    "\n",
    "    \n",
    "        processed_img = cv2.resize(img, (200,400))  \n",
    "        processed_img = processed_img / 255.0  \n",
    "\n",
    "        # Add processed image to the list\n",
    "        train_images.append(processed_img)\n",
    "\n",
    "concatenated_images = np.array(train_images)\n",
    "\n",
    "\n",
    "print(\"Shape of concatenated images:\", concatenated_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8308e2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 17\n",
    "\n",
    "labels = y = np.repeat([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],50)\n",
    "\n",
    "#one-hot encoding\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes)\n",
    "print(y[48:52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ae4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(400, 200, 3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(17, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752881e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6e965f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 107s 4s/step - loss: 1.6997 - accuracy: 0.6165\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 107s 4s/step - loss: 0.0019 - accuracy: 0.9988\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 121s 4s/step - loss: 1.2534e-05 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 130s 5s/step - loss: 5.8466e-06 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 154s 6s/step - loss: 4.0557e-06 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 116s 4s/step - loss: 3.1687e-06 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 111s 4s/step - loss: 2.5521e-06 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 132s 5s/step - loss: 2.1029e-06 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 152s 6s/step - loss: 1.7246e-06 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 133s 5s/step - loss: 1.4432e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19e35563610>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(concatenated_images,y,epochs =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14af9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##storing the model\n",
    "model.save('simple_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ecdc6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n"
     ]
    }
   ],
   "source": [
    "##lIVE USING CAMERA\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from keras.models import load_model\n",
    "import csv\n",
    "\n",
    "model = load_model('simple_cnn.h5')\n",
    "\n",
    "\n",
    "def preprocess_image(frame):\n",
    "    \n",
    "    processed_image = frame / 255.0  \n",
    "    return processed_image\n",
    "\n",
    "\n",
    "attendance = True\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "with open('attendance_log.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Date', 'Time', 'Prediction'])\n",
    "\n",
    "logged_predictions = set() \n",
    "\n",
    "def log_attendance(predicted_class):\n",
    "    if predicted_class not in logged_predictions:  \n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S\")\n",
    "        current_date = now.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        with open('attendance_log.csv', mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([current_date, current_time, predicted_class])\n",
    "\n",
    "        logged_predictions.add(predicted_class)  \n",
    "\n",
    "while attendance:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  ##convert into gray scale\n",
    "\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)) ##for face  detection\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        padding = 50\n",
    "        face_center_x = x + w // 2\n",
    "        face_center_y = y + h // 2\n",
    "        roi = frame[max(0, face_center_y - h // 2 - padding):min(frame.shape[0], face_center_y + h // 2 + padding),   ##region of interest\n",
    "                    max(0, face_center_x - w // 2 - padding):min(frame.shape[1], face_center_x + w // 2 + padding)]\n",
    "\n",
    "        resized_face = cv2.resize(roi, (200, 400))\n",
    "\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2) ##rectangle frame\n",
    "\n",
    "        processed_image = resized_face / 255.0  \n",
    "        prediction = model.predict(np.array([processed_image]))\n",
    "\n",
    "        class_labels = ['J004-Jay Ajmera','J011-Savali Chavan','J012-Snehee Cheeda','J015 -Farid Damania','J016-Aditya Das','J021- Heet Dhandukia','J023-Anish Gharat','J024-Suchetan Ghosh',\n",
    "                        'J025-Monish Gosar','J031- Naiktik Jain','J037-Rudra Joshi','J056-Atharva Rode','J058-Jash Shah','J065- Kallind Soni','J066- Naman Upadhyay','J069- Ismail Wangde','J074- Mihir Shah']\n",
    "        predicted_class = class_labels[np.argmax(prediction)]\n",
    "\n",
    "        log_attendance(predicted_class)\n",
    "\n",
    "        annotation = predicted_class\n",
    "        cv2.putText(frame, annotation, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    frame = cv2.resize(frame, (int(frame_width * (800 / frame_height)), 400))   # frame to match the height of the resized face (2:1 ratio)\n",
    "  \n",
    "\n",
    "    cv2.imshow('Face Detection and Prediction', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13adde09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e3ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706389c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c555dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced9537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7fe0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ae175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91b302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
