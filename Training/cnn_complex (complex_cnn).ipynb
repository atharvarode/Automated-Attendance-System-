{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16b8cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dropout,BatchNormalization,Activation\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "360e4104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of concatenated images: (850, 400, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# List of folders\n",
    "folders = [\"C:/Users/rodea/OneDrive/Desktop/Dataset/J004\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J011\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J012\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J015\",\n",
    "           \"C:/Users/rodea/OneDrive/Desktop/Dataset/J016\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J021\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J023\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J024\",\n",
    "           \"C:/Users/rodea/OneDrive/Desktop/Dataset/J025\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J031\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J037\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J056\",\n",
    "           \"C:/Users/rodea/OneDrive/Desktop/Dataset/J058\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J065\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J066\",\"C:/Users/rodea/OneDrive/Desktop/Dataset/J069\",\n",
    "           \"C:/Users/rodea/OneDrive/Desktop/Dataset/J074\"]\n",
    "        \n",
    "          \n",
    "          \n",
    "\n",
    "# Initialize an empty list to store processed images\n",
    "train_images = []\n",
    "\n",
    "# Iterate through each folder\n",
    "for folder in folders:\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder)\n",
    "\n",
    "    # Iterate through each file\n",
    "    for file in files:\n",
    "        # Get the full file path\n",
    "        file_path = os.path.join(folder, file)\n",
    "\n",
    "        # Read the image\n",
    "        img = cv2.imread(file_path)\n",
    "\n",
    "        # Process the image (e.g., resize, normalize, etc.)\n",
    "        processed_img = cv2.resize(img, (200,400))  #\n",
    "        processed_img = processed_img / 255.0  \n",
    "\n",
    "        # Add processed image to the list\n",
    "        train_images.append(processed_img)\n",
    "\n",
    "# Convert the list of images to a numpy array\n",
    "concatenated_images = np.array(train_images)\n",
    "\n",
    "# Print the shape of concatenated images\n",
    "print(\"Shape of concatenated images:\", concatenated_images.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1629bbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 17\n",
    "labels = y = np.repeat([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],50)\n",
    "\n",
    "\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes)\n",
    "print(y[48:52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3df78b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(400, 200, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(17, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac3305d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23f5aedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 178s 6s/step - loss: 1.4365 - accuracy: 0.6024\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.2705 - accuracy: 0.9576\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 187s 7s/step - loss: 0.1304 - accuracy: 0.9859\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 171s 6s/step - loss: 0.0588 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 173s 6s/step - loss: 0.0477 - accuracy: 0.9988\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 183s 7s/step - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 520s 20s/step - loss: 0.0294 - accuracy: 0.9988\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 167s 6s/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 155s 6s/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 161s 6s/step - loss: 0.0217 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17c0d26a640>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(concatenated_images,y,epochs =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "168e0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##storing the model\n",
    "model.save('complex_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09888e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 918ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n"
     ]
    }
   ],
   "source": [
    "##LIVE USING CAMERA\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from keras.models import load_model\n",
    "import csv\n",
    "\n",
    "\n",
    "model = load_model('complex_cnn.h5')\n",
    "\n",
    "def preprocess_image(frame):\n",
    "    \n",
    "    processed_image = frame / 255.0  \n",
    "    return processed_image\n",
    "\n",
    "\n",
    "attendance = True\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "with open('attendance_log.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Date', 'Time', 'Prediction'])\n",
    "\n",
    "logged_predictions = set()  \n",
    "\n",
    "def log_attendance(predicted_class):\n",
    "    if predicted_class not in logged_predictions:  \n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S\")\n",
    "        current_date = now.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        with open('attendance_log.csv', mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([current_date, current_time, predicted_class])\n",
    "\n",
    "        logged_predictions.add(predicted_class) \n",
    "\n",
    "while attendance:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        \n",
    "        padding = 50\n",
    "        face_center_x = x + w // 2\n",
    "        face_center_y = y + h // 2\n",
    "        roi = frame[max(0, face_center_y - h // 2 - padding):min(frame.shape[0], face_center_y + h // 2 + padding),\n",
    "                    max(0, face_center_x - w // 2 - padding):min(frame.shape[1], face_center_x + w // 2 + padding)]\n",
    "\n",
    "        resized_face = cv2.resize(roi, (200, 400))\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "        processed_image = resized_face / 255.0  \n",
    "        prediction = model.predict(np.array([processed_image]))\n",
    "\n",
    "        class_labels = ['J004-Jay Ajmera','J011-Savali Chavan','J012-Snehee Cheeda','J015 -Farid Damania','J016-Aditya Das','J021- Heet Dhandukia','J023-Anish Gharat','J024-Suchetan Ghosh',\n",
    "                        'J025-Monish Gosar','J031- Naiktik Jain','J037-Rudra Joshi','J056-Atharva Rode','J058-Jash Shah','J065- Kallind Soni','J066- Naman Upadhyay','J069- Ismail Wangde','J074- Mihir Shah']\n",
    "        predicted_class = class_labels[np.argmax(prediction)]\n",
    "\n",
    "        log_attendance(predicted_class)\n",
    "\n",
    "        annotation = predicted_class\n",
    "        cv2.putText(frame, annotation, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    frame = cv2.resize(frame, (int(frame_width * (800 / frame_height)), 400))\n",
    "  \n",
    "\n",
    "    cv2.imshow('Face Detection and Prediction', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da35e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffce1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
